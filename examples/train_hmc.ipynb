{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from manet.hmc import HMC\n",
    "from manet.m3n import M3N, one_hot\n",
    "from manet.mn_models import MrMaNetHomo, AdvMaNetHomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use synthetic data generated from a Hidden Markov Chain of discrete sequences. That is, the observable sequence $(x_0,\\ldots,x_{l-1})\\in \\{0,\\ldots,n_x-1\\}^l$ and the hidden sequence $(y_0,\\ldots,y_{l-1})\\in \\{0,\\ldots,n_y-1\\}^l$ follow the distribution:\n",
    "$$ P(x_0,\\ldots,x_{l-1},y_0,\\ldots,y_{l-1})=P_0(y_0)\\prod_{i=1}^{l-1} P(y_i\\mid y_{i-1}) P(x_i\\mid y_i)$$\n",
    "\n",
    "We set the transition distribution to\n",
    "$$P(y_{i+1}\\mid y_i) = \\left \\{ \\begin{array}{lll}\n",
    "   \\alpha & if & y_{i+1} = y_i\\\\\n",
    "   (1-\\alpha)/(n_y-1) & if & y_{i+1} \\neq y_i\n",
    "  \\end{array}\n",
    "  \\right .\n",
    "$$\n",
    "and the emission probability to\n",
    "$$P(x_i\\mid y_i) = \\left \\{ \\begin{array}{lll}\n",
    "   \\beta & if & x_i = y_i\\\\\n",
    "   (1-\\beta)/(n_x-1) & if & x_i \\neq y_i\n",
    "  \\end{array}\n",
    "  \\right .\n",
    "$$\n",
    "\n",
    "The number of observeble symbols $n_x$, hidden symbols $n_y$, the initial state probability $P_0(y_0)$, the numbers $\\alpha\\in(0,1)$, $\\beta\\in(0,1)$ and the sequence length $l$ are hyperparameters defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 30  # number of observable symbols\n",
    "n_y = 10  # number of hidden symbols\n",
    "alpha = 0.7 \n",
    "beta = 0.7\n",
    "seq_length = 10 \n",
    "\n",
    "# define transition probability\n",
    "trans = (1-beta)*np.ones((n_y,n_y)) / (n_y-1)\n",
    "np.fill_diagonal( trans, beta )\n",
    "\n",
    "# define emission probability\n",
    "emis = (1-alpha)*np.ones( (n_y,n_x )) / (n_x - 1)\n",
    "np.fill_diagonal( emis, alpha )\n",
    "\n",
    "# define initial state probability\n",
    "p0 = np.ones( n_y ) / n_y\n",
    "\n",
    "# initialize HMC class\n",
    "Hmc = HMC( p0, trans, emis, alphabet=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we know the distribution of the data $P(x_0,\\ldots,x_{l-1},y_0,\\ldots,y_{l-1})$, we can construct Bayes optimal rule provided the goals is to minimize the expected Hamming loss \n",
    "$$\n",
    "  \\ell_{Hamming}(y_0,\\ldots,y_{l-1},\\hat{y}_0,\\ldots,\\hat{y}_{l-1}) = \\frac{1}{l}\\sum_{i=1}^{l-1}\\delta(y_i\\neq \\hat{y}_i)\n",
    "$$\n",
    "or the expected 0/1-loss\n",
    "$$\n",
    "  \\ell_{0/1}(y_0,\\ldots,y_{l-1},\\hat{y}_0,\\ldots,\\hat{y}_{l-1}) = \\left \\{\n",
    "    \\begin{array}{ll}\n",
    "      1 & if \\; \\exist y_i\\neq \\hat{y}_i\\\\\n",
    "      0 & otherwise\n",
    "      \\end{array}\n",
    "      \\right .\n",
    "$$\n",
    "In case of the Hamming loss, the Bayes rule is a sequence of the most probable states which can be computed by function <code>HMC.decode</code>. In case of the 0/1l-loss, the Bayes rule is the maximum a posteriory sequence computed which can be computed by function <code>HMC.map</code>. Having the Bayes rules, we estimate the Bayes risk (best attainable) to get a reference solution for later comparison with the rule learned from examples by M3N algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes risk for Hamming loss: 0.15646999999999756\n",
      "bayes risk for 0/1-loss: 0.7467\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "\n",
    "risk_hamming_decode = 0  # Expected Hamming loss of max-marginals (decode) rule\n",
    "risk_01_map = 0          # Expected 0/1-loss of MAP rule\n",
    "for i in range( m ):\n",
    "    X, Y = Hmc.generate( seq_length )\n",
    "\n",
    "    # Maximum aposteriory rule\n",
    "    Ymap, log_map = Hmc.map(X)\n",
    "    risk_01_map += int( np.any( Y != Ymap ) )\n",
    "\n",
    "    # Decode rule i.e. sequnce of the most probable states\n",
    "    P = Hmc.decode( X )\n",
    "    Ydec = np.argmax( P, axis = 0)\n",
    "    risk_hamming_decode += np.count_nonzero( Y-Ydec ) / seq_length\n",
    "    \n",
    "risk_01_map /= m\n",
    "risk_hamming_decode /= m\n",
    "    \n",
    "print(\"Bayes risk for Hamming loss:\", risk_hamming_decode)\n",
    "print(\"bayes risk for 0/1-loss:\", risk_01_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HMC model to generate set of training sequences and testing sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trn_examples = 500\n",
    "n_tst_examples = 500\n",
    "def generate_sequences( Hmc, length, n_examples):\n",
    "    examples = []\n",
    "    for i in range( n_examples):\n",
    "        X, Y = Hmc.generate(  length )\n",
    "        examples.append({'X': X, 'Y': Y, 'n_x': Hmc.n_x, 'n_y': Hmc.n_y} )\n",
    "    return examples        \n",
    "\n",
    "trn_sequences = generate_sequences( Hmc, length=10, n_examples=n_trn_examples)\n",
    "tst_sequences = generate_sequences( Hmc, length=10, n_examples=n_tst_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markov_nets( model, sequences ):\n",
    "    examples = []\n",
    "    for seq in sequences:\n",
    "        length = len( seq['X'])        \n",
    "        edges = np.concatenate(( np.arange(0,length-1).reshape((1,length-1)),\n",
    "                                 np.arange(1,length).reshape((1,length-1)) ),axis=0)\n",
    "        examples.append( model( n_y = seq['n_y'],\\\n",
    "                            X = one_hot(seq['n_x'],seq['X']),\\\n",
    "                            Y = seq['Y'],\\\n",
    "                            E = edges, \\\n",
    "                            graph = 'chain') )\n",
    "    return examples\n",
    "\n",
    "model = MrMaNetHomo\n",
    "#model = AdvMaNetHomo\n",
    "\n",
    "trn_examples = create_markov_nets( model, trn_sequences)\n",
    "tst_examples = create_markov_nets( model, tst_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 obj_val=0.9688188856804634\n",
      "epoch=10 obj_val=0.4894249807526649\n",
      "epoch=20 obj_val=0.42876403991525086\n",
      "epoch=30 obj_val=0.4081299799122288\n",
      "epoch=40 obj_val=0.39791987660025224\n",
      "epoch=50 obj_val=0.3912414273496622\n",
      "epoch=60 obj_val=0.38776856863454234\n",
      "epoch=70 obj_val=0.38529607243497255\n",
      "epoch=80 obj_val=0.3831610896594044\n",
      "epoch=90 obj_val=0.38178697353349267\n",
      "epoch=99 obj_val=0.38103766553834023\n"
     ]
    }
   ],
   "source": [
    "config = {'num_epochs': 100,\n",
    "          'lr_const': 0.01,\n",
    "          'lr_exp': -1,\n",
    "          'eval_obj': 10,\n",
    "          'verb': True,\n",
    "          'solver': 'adam'}\n",
    "\n",
    "algo = M3N( config )\n",
    "\n",
    "lam = 0\n",
    "W, obj_val = algo.train( trn_examples, lam )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training risk of M3N predictor\n",
      "Hamming loss: 0.16539999999999996\n",
      "01-loss: 0.784\n",
      "\n",
      "Test risk of M3N predictor\n",
      "Hamming loss: 0.1772\n",
      "01-loss: 0.812\n",
      "\n",
      "Test risk of Bayes predictor\n",
      "Hamming loss: 0.15646999999999756\n",
      "01 loss: 0.7467\n"
     ]
    }
   ],
   "source": [
    "trn_hamming_loss, trn_01_loss = algo.eval( W, trn_examples )\n",
    "trn_risk_hamming_m3n = np.mean(trn_hamming_loss)\n",
    "trn_risk_01_m3n = np.mean(trn_01_loss)\n",
    "\n",
    "print(\"Training risk of M3N predictor\")\n",
    "print( \"Hamming loss:\",trn_risk_hamming_m3n)\n",
    "print( \"01-loss:\", trn_risk_01_m3n )\n",
    "\n",
    "tst_hamming_loss, tst_01_loss = algo.eval( W, tst_examples )\n",
    "tst_risk_hamming_m3n = np.mean(tst_hamming_loss)\n",
    "tst_risk_01_m3n = np.mean(tst_01_loss)\n",
    "\n",
    "print()\n",
    "print(\"Test risk of M3N predictor\")\n",
    "print( \"Hamming loss:\",tst_risk_hamming_m3n)\n",
    "print( \"01-loss:\", tst_risk_01_m3n)\n",
    "\n",
    "print()\n",
    "print(\"Test risk of Bayes predictor\")\n",
    "print( \"Hamming loss:\", risk_hamming_decode)\n",
    "print( \"01 loss:\", risk_01_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: 0\n",
      "predictor score   : 2.956114103988624\n",
      "input sequnce     : [18  2 25  3  2  8  9  5  5  5]\n",
      "true sequence     : [2 2 2 2 2 8 9 5 5 5]\n",
      "predicted sequence: [2 2 3 3 2 8 9 5 5 5]\n",
      "example: 1\n",
      "predictor score   : 3.552639619466107\n",
      "input sequnce     : [ 1  1  1  1  2  2 15  7  7  7]\n",
      "true sequence     : [1 1 1 1 2 2 2 7 7 7]\n",
      "predicted sequence: [1 1 1 1 2 2 2 7 7 7]\n",
      "example: 2\n",
      "predictor score   : 3.1590741650319796\n",
      "input sequnce     : [ 9  9  9 28  1 19  7  4  4  2]\n",
      "true sequence     : [9 9 9 1 1 7 7 4 2 2]\n",
      "predicted sequence: [9 9 9 1 1 7 7 4 4 2]\n",
      "example: 3\n",
      "predictor score   : 3.0384863338625823\n",
      "input sequnce     : [ 7  7  1 12 20  7  7  8 24  1]\n",
      "true sequence     : [7 7 7 7 7 7 7 8 5 1]\n",
      "predicted sequence: [7 7 1 1 1 7 7 8 8 1]\n",
      "example: 4\n",
      "predictor score   : 3.1639417626578994\n",
      "input sequnce     : [ 1  1  8  8  4  8  8  0 19  5]\n",
      "true sequence     : [1 1 8 8 8 8 8 0 0 0]\n",
      "predicted sequence: [1 1 8 8 4 8 8 0 5 5]\n",
      "example: 5\n",
      "predictor score   : 3.0762611876190893\n",
      "input sequnce     : [ 6 24  9  9  2 28 29  2  2  6]\n",
      "true sequence     : [6 2 9 9 2 2 2 2 2 2]\n",
      "predicted sequence: [6 6 9 9 2 2 2 2 2 6]\n",
      "example: 6\n",
      "predictor score   : 3.1675205043467107\n",
      "input sequnce     : [ 1  1 16  4  4  9  9  7 12  5]\n",
      "true sequence     : [1 1 1 4 4 9 9 9 9 5]\n",
      "predicted sequence: [1 1 1 4 4 9 9 7 5 5]\n",
      "example: 7\n",
      "predictor score   : 3.2462429729725355\n",
      "input sequnce     : [ 5  5  5  5  5 15 23  5  5 25]\n",
      "true sequence     : [5 5 5 5 5 5 5 5 5 5]\n",
      "predicted sequence: [5 5 5 5 5 5 5 5 5 5]\n",
      "example: 8\n",
      "predictor score   : 2.8810346946045176\n",
      "input sequnce     : [ 0  0 12  9 12  2 25  4 20  4]\n",
      "true sequence     : [0 0 0 9 2 2 4 4 4 4]\n",
      "predicted sequence: [0 0 9 9 2 2 4 4 4 4]\n",
      "example: 9\n",
      "predictor score   : 3.200188729900461\n",
      "input sequnce     : [ 0  0  0  0  0 22 27  0  3  5]\n",
      "true sequence     : [0 0 0 0 0 0 0 0 3 3]\n",
      "predicted sequence: [0 0 0 0 0 0 0 0 3 5]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    Y_pred, score = algo.predict( W, tst_examples[i] )\n",
    "\n",
    "    print(\"example:\", i)\n",
    "    print(\"predictor score   :\", score )\n",
    "    print(\"input sequnce     :\", tst_sequences[i]['X'])\n",
    "    print(\"true sequence     :\", tst_sequences[i]['Y'] )\n",
    "    print(\"predicted sequence:\",Y_pred )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('manet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "34eb675ba1599e41a978d852241fe8b4873ed3c3e59b32810c909d7852b5516c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
